---
title: "classifier_development"
author: "Sergi"
date: "1/14/2020"
output: pdf_document
abstract: in the previous projects 3.3.1 and 3.3.2 I have developed classifiers to filter out wrongly segmented cells and to classify them into Crowded and Isolated based on the local cell density around the cell. In the current project I will train an algorithm to classify isolated cells into morphological groups based on the image profile. The approach will be very similar to the time-resolved pipeline. However in this case, the mitosis group will be renamed as irregular nucleus, because it seemed that not only mitotic cell fall into this categories but other states such as polynucleated cells or in general aberrations in the nuclear morphology. Initially, I will include metaphase cells inside of this group (strong signal in the tubulin channel reflectin the nuclear spindle), because there are very few examples of cells belonging to this group. Furthermore, we don not expect RNAi conditions to result in an increase of cells in this state due to is transient nature. 
Reinforced learning and model probabilities will be used during the generation of the training set. 
editor_options: 
  chunk_output_type: console
---
#Package loading
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(dbplyr)
library(corrr)
library(caret)
library(umap)
library(randomForest)
library(EBImage)
library(e1071)
library(RColorBrewer)
library(plotly)
library(Rtsne)
library(jsonlite)
library(kernlab)
library(RSQLite)
library(rlist)
library(plyr)
library(LMGene)
library(gridExtra)
library(ggpubr)
library(uwot)
library(kernlab)
library(nnet)
library(SuperLearner)
library(KernelKnn)
library(ranger)
library(ISLR)

load("objects/numeric_features.RData")

```
#Image filtering
In order to generate the training set, I will use the same single cell images as in the previous classifiers. However, in this case I am only interested in well-segmented isolated cells. Therefore, cells which do not meet this requirements should not be shown in the labelling app. In order to avoid computing this step when loading images in the shiny app (it makes the app slower), I will prefilter the images and copy them into a folder. 
```{r}

filtering_classifier <- readRDS("objects/filtering_classifier.rds")

density_classifier <- readRDS("objects/density_classifier.rds")

#Here is the folder with all the single cell objects. There are almost 100,000 cells. 

input_folder <- "~/Desktop/Sergi_master_thesis/Aim3_large_screen_analysis/3.2_data_preparation/3.2.1_images_classifiers/single_cell_images/"

#This is the folder where I will copy the objects from well segmented isolated cells. 

output_folder <- "single_cell_isolated_images/"

#Since I am running the loop, I will also save crowded cells in a different folder, so that the code does not need to be run again when the crowded classifier is trained. 

output_folder_crowded <- "~/Desktop/Sergi_master_thesis/Aim3_large_screen_analysis/3.3_classifier_development/3.3.4_state_crowded_classifier/single_cell_crowded_objects/"

pb <- txtProgressBar(min = 0, max = length(list.files(input_folder)), style = 3)

count <- 0

for(file in list.files(input_folder)){
  
  count <- count+1
  
  setTxtProgressBar(pb, count)
  
  if(predict(filtering_classifier, readRDS(paste0(input_folder, file))$features) == "Yes" &
     predict(density_classifier, readRDS(paste0(input_folder, file))$features) == "Isolated"){
    
    file.copy(from = paste0(input_folder, file), to = output_folder, overwrite = F)
    
  }else if (predict(filtering_classifier, readRDS(paste0(input_folder, file))$features) == "Yes" &
     predict(density_classifier, readRDS(paste0(input_folder, file))$features) == "Crowded"){
    
    
    file.copy(from = paste0(input_folder, file), to = output_folder_crowded, overwrite = F)
    
     }
}

#Only around 30,000 of the initial 96,000 cells are well segmented and isolated.  

```
#Initial training set
I manually labelled 1500 cells into the groups: Big, Condensed, Elongated, Irregular_nucleus and Normal. I used reinforced learning from the classification 
of 150 cells. However, at the end I went back to choosing random cells and only labelling those which I could clearly distinguish as belonging to one of the
classes. 
##Cell distribution
```{r}

initial_training_set <- readRDS("objects_classification/before_cleanup/initial_training_set.rds") %>% 
                          mutate(group = factor(group, levels = c("Big", "Condensed", "Elongated", "Irregular_nucleus", "Normal")))


initial_random_forest <- readRDS("objects_classification/before_cleanup/initial_classifier_rf.rds")

#I trained a random forest as I classified cells and it showed an accuracy close to 76 %. The elongated class is the one with lowest classification accuracy. 

varImpPlot(initial_random_forest)

#the top features regarding importance are actin features which describe the cell body as well as DNA and actin eccentricity. This there are no tubulin 
# features among the most important ones. 


imp_feat_initial_rf <- initial_random_forest$importance %>% as.data.frame() %>% rownames_to_column(var = "feature") %>% arrange(desc(MeanDecreaseAccuracy))
    
#I will pick the 20 most important features for the model. 
    
imp_feat_initial_rf <- imp_feat_initial_rf[1:20,1]

##################################################################################################################################################

# Numbers table

pdf("objects_classification/before_cleanup/plots/numbers_table.pdf")
grid.table(readRDS("objects_classification/before_cleanup/numbers_table.rds"), row = NULL)
dev.off()

##################################################################################################################################################

# UMAP Important features

google_green <- "#3cba54"

google_blue <- "#4885ed"

google_yellow <- "#f4c20d"

google_red <- "#db3236"

lacoste_green <- "#004526"

apple_grey <- "#7d7d7d"

colours <- c(google_green, lacoste_green, google_yellow, google_red, google_blue)

load("/Users/b110/Desktop/Sergi_master_thesis/themes/umap_theme.RData")

set.seed(100)
umap_init_imp <- initial_training_set %>% select(imp_feat_initial_rf) %>% uwot::umap(n_neighbors = 30) %>% as.data.frame() %>% 
                    dplyr::rename(dimension1 = V1, dimension2 = V2) %>%  mutate(group = initial_training_set %>% pull(group))


umap_init_imp_plot <- ggplot(umap_init_imp, aes(x=dimension1, y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = colours, labels = c("Big", "Condensed", "Elongated", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_init_imp_plot, filename = "objects_classification/before_cleanup/plots/umap_init_imp.pdf", device = "pdf", height = 8, width = 8.5)

#As expected we get a continuum of phenotypes. Normal cells are distributed all over the map. Condensed cells seem to be quite separate from the group of big
#elongated and irregular nucleus. The overall distribution of cells is very similar to the one obtained for the time-resolved data. 

#UMAP Normal in grey

#Since normal cells are scattered throughout the plot it is convenient to paint them in grey so that the distribution of the other groups (the ones we are 
#intersted in) can be more clearly seen. 

umap_init_imp_plot <- ggplot(umap_init_imp, aes(x=dimension1, y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = c(google_green, lacoste_green, google_yellow, google_red, apple_grey),
                                               labels = c("Big", "Condensed", "Elongated", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_init_imp_plot, filename = "objects_classification/before_cleanup/plots/umap_init_imp_grey.pdf", device = "pdf", height = 8, width = 8.5)




```
##Model development
I will train different machine learning algorithms on the data
###Random forest
```{r}

#I will use 5-fold cross-validation as a method to optimise the mtry parameter of the random forest

control <- trainControl(method = "cv", number = 5, verboseIter = T, savePredictions = T)

rf_grid <- expand.grid(mtry = c(4:25))

set.seed(100)
rf_initial_optimized <- train(group~., data = initial_training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                              tuneGrid = rf_grid, verbose = T)

#The accuracy for mtry = 10 is 76.7 %

#I will try adding more trees to the forest.

set.seed(100)
rf_initial_optimized <- train(group~., data = initial_training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                              tuneGrid = rf_grid, verbose = T, ntree = 1000)

#There does not seem to be much difference. The accuracy of the best model is 76.8 %. 

```
###Svm
SVM are also suitable for multivariate classification. I will test if the performance is better than the random forest. 
```{r}

#SVM are very sensitive to redundant features, therefore I will train the algorithm only on the most important features for the classifier. 

set.seed(100)
svm_radial <- train(group~., data = initial_training_set %>% select(group, imp_feat_initial_rf), trControl = control, method = "svmRadial", 
                         tuneGrid  = expand.grid(C =c(0.25, 0.5, 0.75,1,1.5,2,5,10,15,20), sigma = seq(0.01,0.2, by = 0.025)))

#The highest accuracy is 73.7 %

```
##Confusion matrix
A random forest with mtry = 12 was the most accurate algorithm. I will make a confusion matrix to investigate the classification error of the different groups. 
```{r}

conf_optimal_rf <- table(rf_initial_optimized$pred$obs, rf_initial_optimized$pred$pred) %>% as.data.frame() %>% arrange(Var1, Var2)

    proportion <- vector()

    for(row in 1:nrow(conf_optimal_rf)){

         group <- conf_optimal_rf[row,1]

         total_cells <- sum(conf_optimal_rf %>% filter(Var1 ==group) %>% select(Freq))

         row_proportion <- conf_optimal_rf[row, "Freq"]/total_cells

         proportion <- c(proportion, row_proportion)

    }
  

conf_optimal_rf <- conf_optimal_rf %>% mutate(proportion_cells=proportion,
                                              Var2 = factor(Var2, levels = c("Normal", "Irregular_nucleus", "Elongated", "Condensed", "Big")))

load("~/Desktop/Sergi_master_thesis/themes/confusion_matrix_theme.RData")

confusion_matrix_initial_rf <- ggplot(conf_optimal_rf, aes(x = Var1, y = Var2, fill = proportion_cells))+
                                 geom_tile()+
                                 scale_x_discrete(expand = c(0,0), labels = c("Big", "Condensed", "Elongated", "Irregular\nnucleus", "Normal"))+
                                 scale_y_discrete(expand = c(0,0), labels = c("Normal", "Irregular\nnucleus", "Elongated", "Condensed", "Big"))+
                                 scale_fill_gradient(low = "white", high = "#4885ed")+
                                 xlab("user-defined group")+
                                 ylab("predicted group")+
                                 ggtitle("Confusion matrix (proportion of cells)")+
                                 confusion_matrix_theme()+
                                 #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
                                 geom_label(data = conf_optimal_rf %>% filter(proportion_cells>0.1), 
                                            aes(x = Var1, y = Var2, label = round(proportion_cells, 2)), size = 6, label.size = 0.5)

ggsave(confusion_matrix_initial_rf, filename = "objects_classification/before_cleanup/plots/confusion_matrix_rf.pdf", height = 7, width = 10)
       
#All classes have between 75-80 % accuracy. Nevertheless, the elongated is poorly predicted with an classification error of 60 %. 
```
##Repeated cross-validation
I will carry out 10 times 5-fold cross-validation in order to determine which cells in the training set are poorly predicted by the model. Then I will select cells that were missclassified in 3 or more of the replicates. These cells will be manually re-inspected and they could be relabelled or discarded for the final training set. By doing, this clean-up state I hope to obtain a more clear separation between the classes which could make it easier for the classifier to assign labels. 
```{r}

control_rep_cv <- trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = T, savePredictions = "final")

rf_initial_rep_cv <- train(group~., data = initial_training_set %>% select(group, numeric_features),
                           trControl = control_rep_cv, method = "rf",tuneGrid=expand.grid(mtry=c(12)), ntree = 1000)

#We can see the predictions of the final model during the repeated cross_validation process. 

rf_pred_cv <- rf_initial_rep_cv$pred %>% as.data.frame() %>% arrange(rowIndex)

saveRDS(rf_pred_cv, file = "objects_classification/before_cleanup/predictions_rep_cv_rf.rds")

#I will get the cells which were wrongly predicted in 3 or more of the cross-validation repetitions. This constitutes 27.4 % of the cells.

wrong_pred_cells <- rf_pred_cv %>% filter(pred != obs) %>% count("rowIndex") %>% filter(freq>2) %>% pull(rowIndex)

#There are 413 cells which are wrongly assigned in three or more repetitions. 

#I would like to save this object in order to use it in the relabelling shinny app

saveRDS(wrong_pred_cells, file = "objects_classification/before_cleanup/wrong_predictions_indexes.rds")

#For some reason shiny cannot open the file, so I will save as an RData file. 

save(wrong_pred_cells, file = "objects_classification/before_cleanup/wrong_predictions_indexes.RData")

```
#Clean training set
I manually inspect 413 cells which were wrongly classified three or more times by the models in 10 times 5-fold cross-validation. The resulting dataset will be used as the final training set to obtain the isolated morphology classifier. 
##Relabelling table
```{r}

relabel_numbers <- readRDS("objects_classification/after_cleanup/numbers_cells.rds")

#I relabelled 25 % of the inspected cells (n = 105, 7 % of the initial cells of the training set).
#I discarded 31.8 % of the inspected cells (n = 131, 8.72 % of the initial cells of the training set).
#I manually inspected 321 cells out of 1200. I relabelled 62 of the cells and discarded 137. 

relabelled_cells_table <- relabel_numbers %>% mutate(proportion = round(number_cells/15.02, 2)) %>% select(group, proportion, number_cells)

pdf(file = "objects_classification/after_cleanup/plots/relabelling_numbers.pdf")
grid.table(relabelled_cells_table, rows = NULL)
dev.off()

#I modified around 15% of the original dataset. 

relabelled_class_prop_table <- readRDS("objects_classification/before_cleanup/numbers_table.rds") %>% dplyr::rename(initial_number = number_cells) %>%
                                arrange(group) %>% 
                                mutate(after_relabel = c(count(clean_training_set %>% filter(group != "Discard"), "group")$freq,nrow(clean_training_set %>% 
                                filter(group != "Discard")))) %>% select(group, initial_number, after_relabel) %>% 
                                mutate(difference = round((after_relabel-initial_number)/initial_number*100,2))

#After relabelling the training set is much much more balanced. The normal class still consitutes the majority but it only has 2x more instances than other groups such as condensed and irregular nucleus. 


pdf(file = "objects_classification/after_cleanup/plots/class_numbers.pdf")
grid.table(relabelled_class_prop_table, row = NULL)
dev.off()
```
##Cell distribution
```{r}

clean_training_set <- readRDS("objects_classification/after_cleanup/training_set_cleaned.rds") %>% filter(group != "Discard") %>% 
                        mutate(group = factor(group))

#26 cells were eliminated from the training set because after retraining the filtering and density classifiers the labels were wrong. 

clean_training_set <- clean_training_set %>% filter(predict(filtering_classifier, clean_training_set) == "Yes",
                                                    predict(density_classifier, clean_training_set) == "Isolated")
                        

#Basic random forest

clean_rf <- randomForest(group~., data = clean_training_set %>% select(group, numeric_features), importance = T)

#The accuracy has increased to 86.8 %. However, the elongated class is still the one with a lower classification accuracy just below 80 %. 

varImpPlot(clean_rf)

#The important features for the classifier seem to be very similar to the initial random forest. Only one of the top most important features changes. 


#####################################################################################################################################################

#UMAP

set.seed(100)
umap_clean_imp <- clean_training_set %>% select(imp_feat_initial_rf) %>% uwot::umap(n_neighbors = 30) %>% as.data.frame() %>% 
                    dplyr::rename(dimension1 = V1, dimension2 = V2) %>%  mutate(group = clean_training_set %>% pull(group))


umap_clean_imp_plot <- ggplot(umap_clean_imp, aes(x=(dimension1), y= dimension2, 
                                                  color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = colours, 
                                               labels = c("Big", "Condensed", "Elongated",
                                                          "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, 
                                                         override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_clean_imp_plot, filename = "objects_classification/after_cleanup/plots/umap_imp.pdf", height = 8, width = 8.5)


#UMAP with normal group in grey. 

set.seed(100)
umap_clean_imp_grey <- clean_training_set %>% select(imp_feat_initial_rf) %>% uwot::umap(n_neighbors = 30) %>% as.data.frame() %>% 
                    dplyr::rename(dimension1 = V1, dimension2 = V2) %>%  mutate(group = clean_training_set %>% pull(group))


umap_clean_imp_grey_plot <- ggplot(umap_clean_imp, aes(x=(dimension1), y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = c(google_green, lacoste_green, google_yellow, google_red, apple_grey),
                                               labels = c("Big", "Condensed", "Elongated", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_clean_imp_grey_plot, filename = "objects_classification/after_cleanup/plots/umap_imp_grey.pdf", height = 8, width = 8.5)



```
##Model development
I will try to optimise a few hyperparameters of the supervised learning models in order to train the most accurate algorithm possible. 
###Random forest
```{r}

#I will use 5-fold cross-validation in order to get the accuracy of the model.  

control <- trainControl(method = "cv", number = 5, verboseIter = T, savePredictions = T)

rf_grid <- expand.grid(mtry = c(4:25))

set.seed(100)
rf_clean_optimized <- train(group~., data = clean_training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                            tuneGrid = rf_grid, verbose = T)

#The accuracy has increased to 87.4 %. The optimal mtry is 9. 

#The optimal classifier will then be a random forest. I will use the train and 5-fold cross-validation to develop the algorithm. 
#It just makes it easier to explain how the accuracy of the model was calculated with cross-validation than the bagging of the random forest.

set.seed(100)
rf_clean_optimized <- train(group~., data = clean_training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                            tuneGrid = expand.grid(mtry = 9), verbose = T)



```
###SVM
We can train a multiclass SVM on the data
```{r}
# RADIAL SVM on the training set. I will select only the informative features as the performance of SVMs is affected by redundant features. 

control <- trainControl(method = "cv", verboseIter = T, number = 5)

set.seed(100)
svm_radial_clean <- train(group~., data = clean_training_set %>% select(group, imp_feat_initial_rf), trControl = control, method = "svmRadial", 
                         tuneGrid  = expand.grid(C =c(0.75,1,1.5,2,5,10,15,20), sigma = seq(0.01,0.2, by = 0.025)))

#A svm performs much poorly. It only reaches 83.3 % accuracy.  

```
###Confusion matrix
I will generate the confusion matrix for the optimized random forest. 
```{r}

saveRDS(rf_clean_optimized, "objects_classification/after_cleanup/isolated_state_classifier.rds")

conf_optimal_rf <- table(rf_clean_optimized$pred$obs, rf_clean_optimized$pred$pred) %>% as.data.frame() %>% arrange(Var1, Var2) %>% group_by(Var1) %>% 
                      mutate(proportion = Freq/sum(Freq), Var2 = factor(Var2, levels = c("Normal", "Irregular_nucleus", "Elongated", "Condensed", "Big")))


confusion_matrix_initial_rf <- ggplot(conf_optimal_rf, aes(x = Var1, y = Var2, fill = proportion))+
                                 geom_tile()+
                                 scale_x_discrete(expand = c(0,0), labels = c("Big", "Condensed", "Elongated", "Irregular\nnucleus", "Normal"))+
                                 scale_y_discrete(expand = c(0,0), labels = c("Normal", "Irregular\nnucleus", "Elongated", "Condensed", "Big"))+
                                 scale_fill_gradient(low = "white", high = "#4885ed")+
                                 xlab("user-defined group")+
                                 ylab("predicted group")+
                                 ggtitle("Confusion matrix (proportion of cells)")+
                                 confusion_matrix_theme()+
                                 #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
                                 geom_label(data = conf_optimal_rf %>% filter(proportion>0.1), 
                                            aes(x = Var1, y = Var2, label = round(proportion, 2)), size = 6, label.size = 0.5)
       

ggsave(confusion_matrix_initial_rf, filename = "objects_classification/after_cleanup/plots/confusion_matrix_rf.pdf", height = 7, width = 10)

#Elongated cells are still not classified with high-confidence (around 81 % accuracy) but the rest of the groups are quite well predicted after manual reinspection. (accuracy of at least 85 %)




```

#Supp Figures thesis

In the supplementaries of the thesis I will add a plot for every classification algorithm, which will include the confusion matrix and a table of the training set composition. In case I did relabelling it could be interesting to add the initial and final confusion matrices with the corresponding training set tables. 


I will make two tables one which will show the initial group composition of the training set and the final. The other showing the percentage of relabeled and discarded cells after the reinspection step. 

For the Crowded and Isolated Classifiers it might be interesting to also include a UMAP with the group distribution of the cells. 

```{r}


umap_theme_thesis <- function(){
  
  theme_classic()+
    theme(legend.text = element_text(size = 19),
          axis.title.x = element_text(size = 16),
          axis.title.y = element_text(size = 16),
          legend.position = "bottom",
          legend.spacing.x = unit(1, "cm"),
          plot.title = element_blank(),
          axis.title = element_text(size = 13),
          legend.title = element_blank())

}

init_numb_traning <- readRDS("objects_classification/before_cleanup/numbers_table.rds")

relab_numb <- readRDS("objects_classification/after_cleanup/numbers_cells.rds")



#Labelling table

label_table <- relab_numb %>% select(group, number_cells) %>% 
                mutate(Proportions = round(number_cells/1502, 2))

colnames(label_table) <- c(" ", "Number cells", "Proportion")

rownames(label_table) <- NULL


#Training set composition table

train_table <- relabelled_class_prop_table %>% select(-difference) %>% 
                mutate(group = if_else(group == "Irregular_nucleus", "Irregular nucleus", group))

colnames(train_table) <- c("Group", "Initial", "Final")


supp_figure_arrnged <- grid.arrange(arrangeGrob(confusion_matrix_initial_rf + 
                                                  theme(legend.position = "bottom") +
                                                guides(fill = guide_colourbar(
                                                                       title.hjust = 0.5,
                                                                       title.vjust = 0.9,
                                                                      barwidth = unit(5, "cm"),
                                                                      ))+
                                                labs(fill = "Proportion of cells")),
                                    arrangeGrob(tableGrob(label_table, rows = NULL, 
                                                   theme = ttheme_default(base_size = 15)),
                                                tableGrob(train_table, rows = NULL, 
                                                   theme = ttheme_default(base_size = 15)),
                                                nrow = 2), ncol = 2, widths = c(1.5, 1))
                                    
supp_figure_arr <-grid.arrange(arrangeGrob(supp_figure_arrnged),
                                   arrangeGrob(umap_clean_imp_plot+
                                                 theme(legend.title = element_blank())+
                                     guides(colour = guide_legend(nrow = 1, byrow = T,
                                                                override.aes = list(size = 5)))+
                                     umap_theme_thesis()), nrow = 2, heights = c(1,0.9)) 

ggsave(supp_figure_arr, filename = "plots_thesis/supp_figure9_conf.pdf", height = 13, 
       width = 13)

#The UMAP looks awful cause it spans the figure width. That is why I'll make an arranged figure in
#which the UMAP looks better but the conf. matrix and the tables do not. Then I will arrange them 
#inkscape.

supp_figure_arrnged_umap <-grid.arrange(arrangeGrob(supp_figure_arrnged),
                                   arrangeGrob(umap_clean_imp_plot+
                                                 theme(legend.title = element_blank())+
                                     guides(colour = guide_legend(nrow = 1, byrow = T,
                                                                override.aes = list(size = 5)))+
                                     umap_theme_thesis()), nrow = 2, heights = c(1,0.9),
                                   respect = T)  



ggsave(supp_figure_arrnged_umap, filename = "plots_thesis/supp_figure9_umap.pdf", height = 13, 
       width = 13)



```

#Defence figures

```{r}

#I want the training set table only with the final number of cells. 

train_table_final_only <- train_table %>% select(-Initial) %>% 
                            dplyr::rename('Number cells' = Final) %>% 
                            pivot_wider(names_from = Group, values_from = 'Number cells')

pdf("plots_thesis/training_set_numbers.pdf")
grid.table(train_table_final_only)
dev.off()

#I want to show the UMAP plot of the training set but with the legend in two rows

ggsave(umap_clean_imp_plot + theme(legend.title = element_blank()) +
         guides(colour = guide_legend(nrow = 2, byrow = T,override.aes = list(size = 5))),
       height = 7, width = 7.5, filename = "plots_thesis/umap_training_set.pdf")


```

