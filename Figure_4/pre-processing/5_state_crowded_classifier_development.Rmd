---
title: "classifier_development"
author: "Sergi"
date: "1/14/2020"
output: pdf_document
abstract: in the previous projects 3.3.1 and 3.3.2 I have developed classifiers to filter out wrongly segmented cells and to classify them into Crowded and Isolated based on the local cell density around the cell. In the current project I will train an algorithm to classify crowded cells into morphological groups based on the image profile. The approach will be very similar to the time-resolved pipeline. However in this case, the mitosis group will be renamed as irregular nucleus, because it seemed that not only mitotic cell fall into this categories but other states such as polynucleated cells or in general aberrations in the nuclear morphology. Initially, I will include metaphase cells inside of this group (strong signal in the tubulin channel reflectin the nuclear spindle), because there are very few examples of cells belonging to this group. Furthermore, we don not expect RNAi conditions to result in an increase of cells in this state due to is transient nature. 
editor_options: 
  chunk_output_type: console
---
#Package loading
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(dbplyr)
library(corrr)
library(caret)
library(umap)
library(randomForest)
library(EBImage)
library(e1071)
library(RColorBrewer)
library(plotly)
library(Rtsne)
library(jsonlite)
library(kernlab)
library(RSQLite)
library(rlist)
library(plyr)
library(LMGene)
library(gridExtra)
library(ggpubr)
library(uwot)
library(kernlab)
library(nnet)
library(SuperLearner)
library(KernelKnn)
library(ranger)
library(ISLR)

load("objects/numeric_features.RData")

density_classifier <- readRDS("objects/density_classifier.rds")

filtering_classifier <- readRDS("objects/filtering_classifier.rds")

```
#Initial training set
I manually labelled 850 cells into Big, Condensed, Irregular_nucleus and Normal. Surprisingly the classifier accuracy was very high from the beginning, close to 85 %. Big cells are the class with lower classification efficiency, but after labelling it was around 80 %. 
##Cell distribution
```{r}

training_set <- readRDS("objects_classification/training_set.rds") %>% 
                  mutate(group = factor(group, levels = c("Big", "Condensed", "Irregular_nucleus", "Normal")), context = "Crowded")

#19 cells are eliminated from the training set due to the modification of the filtering and density classifiers. 

training_set <- training_set %>% filter(predict(filtering_classifier, training_set) == "Yes", 
                                        predict(density_classifier, training_set) == "Crowded")


initial_random_forest <- readRDS("objects_classification/initial_classifier.rds")

#I trained a random forest as I classified cells and it showed an accuracy close to 76 %. The elongated class is the one with lowest classification accuracy.

varImpPlot(initial_random_forest)

#the top features regarding importance are actin features which describe the cell body as well as DNA and actin eccentricity. This there are no tubulin 
# features among the most important ones. 


imp_feat_initial_rf <- initial_random_forest$importance %>% as.data.frame() %>% rownames_to_column(var = "feature") %>% arrange(desc(MeanDecreaseAccuracy))
    
#I will pick the 20 most important features for the model. 
    
imp_feat_initial_rf <- imp_feat_initial_rf[1:20,1]

##################################################################################################################################################

# Numbers table

numbers_table <- training_set %>% select(group) %>% group_by(group) %>% tally() %>% dplyr::rename(number_cells = n) %>% select(number_cells, group) %>% 
                    mutate(group= as.character(group))

numbers_table[5,] <- c(sum(numbers_table$number_cells), "Total")

pdf("objects_classification/numbers_table.pdf")
grid.table(numbers_table)
dev.off()

##################################################################################################################################################

# UMAP Important features

google_green <- "#3cba54"

google_blue <- "#4885ed"

google_yellow <- "#f4c20d"

google_red <- "#db3236"

lacoste_green <- "#004526"

apple_grey <- "#7d7d7d"

colours <- c(google_green, lacoste_green, google_red, google_blue)

load("/Users/b110/Desktop/Sergi_master_thesis/themes/umap_theme.RData")

set.seed(100)
umap_imp <- training_set %>% select(imp_feat_initial_rf) %>% uwot::umap(n_neighbors = 50) %>% as.data.frame() %>% 
                    dplyr::rename(dimension1 = V1, dimension2 = V2) %>%  mutate(group = training_set %>% pull(group))


umap_imp_plot <- ggplot(umap_imp, aes(x=dimension1, y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = colours, labels = c("Big", "Condensed", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_imp_plot, filename = "objects_classification/plots/umap_imp.pdf", device = "pdf", height = 8, width = 8.5)

#In the UMAP there is a tiny group of cells which are very distant from the cloud. Perhaps making a t-SNE we get a more homogenous picture. 

#######################################################################################################################################################

#t-SNE

t_sne <- training_set %>% select(imp_feat_initial_rf) %>% Rtsne(dims = 2, verbose = T, check_duplicates = F, perplexity = 50) %>% .$Y %>% as.data.frame() %>% 
            dplyr::rename(dimension1 = V1, dimension2 = V2) %>%  mutate(group = training_set %>% pull(group))

tsne_imp_plot <- ggplot(t_sne, aes(x=dimension1, y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = colours, labels = c("Big", "Condensed", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("t-SNE dimension 1")+
                            ylab("t-SNE dimension 2")+
                            labs(color = "Group")

# I still get the same cells as outliers in the map...

            

#UMAP Normal in grey

#Since normal cells are scattered throughout the plot it is convenient to paint them in grey so that the distribution of the other groups (the ones we are 
#intersted in) can be more clearly seen. 

umap_imp_plot_grey <- ggplot(umap_imp, aes(x=dimension1, y= dimension2, color = factor(group)))+
                            geom_point(size = 2)+
                            umap_theme()+
                            theme(
                            legend.title = element_text(size = 16))+
                            scale_color_manual(values = c(google_green, lacoste_green, google_red, apple_grey),
                                               labels = c("Big", "Condensed", "Irregular nucleus", "Normal"))+
                            guides(colour = guide_legend(nrow = 2, byrow = T, override.aes = list(size = 6)))+
                            xlab("UMAP dimension 1")+
                            ylab("UMAP dimension 2")+
                            labs(color = "Group")

ggsave(umap_imp_plot_grey, filename = "objects_classification/plots/umap_imp_grey.pdf", device = "pdf", height = 8, width = 8.5)




```
###Outliers
There are a few cells in the training set which are outliers in the map. I would like to look at the images of these cells to see if a pattern can be spot.
```{r}

outliers_indexes <- which(umap_imp$dimension2>3)

outliers <- training_set[outliers_indexes,]

count(outliers, c("screen", "plate"))

#Most of them come from screen 115. The query is Hiw and the quality report was fine. I could be a plate effect. 

query_genes <- readRDS("~/Desktop/Sergi_master_thesis/gene_annotation/query_genes.rds")

target_genes <- readRDS("~/Desktop/Sergi_master_thesis/gene_annotation/target_genes.rds")

saveRDS(outliers, "objects_classification/outliers_training_set_images/outliers_data.rds")

#I will put the single cell images in a list and then I will tile and combine them. 

list_images <- list()

images_folder <- "single_cell_crowded_images/"

for (cell in 1:nrow(outliers)){
  
  cell_info <- outliers %>% slice(cell)
  
  file_name <- paste0("ERC_20x_4t_SYNA_", cell_info %>% unite(name, plate, screen, well, field, number, sep = "_") %>% pull(name), ".rds")
  
  list_images[["Dapi"]][[cell]] <- readRDS(paste0(images_folder, file_name))$dapi*2
  
  list_images[["Actin"]][[cell]] <- readRDS(paste0(images_folder, file_name))$actin*2
  
  list_images[["Tubulin"]][[cell]] <- readRDS(paste0(images_folder, file_name))$tubulin*2
  
  list_images[["Cells"]][[cell]] <- EBImage::combine(readRDS(paste0(images_folder, file_name))$dapi,
                                                     readRDS(paste0(images_folder, file_name))$actin,
                                                     readRDS(paste0(images_folder, file_name))$tubulin)
  
}


tile(EBImage::combine(list_images[["Dapi"]]), fg.col = "white") %>% display()

tile(EBImage::combine(list_images[["Actin"]]), fg.col = "white") %>% display()

#There does not seem to be a clear pattern in the morphology of the cells...

tile(EBImage::combine(list_images[["Cells"]]), fg.col = "white", nx = 12) %>% display()

#By comparing the nucleus and cell body of the cells one can not distinguish a clear feature that these cells might share...

#I will save the tiled images. 

      writeImage(rgbImage(blue = tile(EBImage::combine(list_images[["Dapi"]]), fg.col = "white", nx = 6),
                          red = tile(EBImage::combine(list_images[["Actin"]]), fg.col = "white", nx = 6),
                          green = tile(EBImage::combine(list_images[["Tubulin"]]), fg.col = "white", nx = 6)),
                 files = c("objects_classification/outliers_training_set_images/actin.tif",
                           "objects_classification/outliers_training_set_images/tubulin.tif",
                           "objects_classification/outliers_training_set_images/dapi.tif"))

```
##Model development
I will train different machine learning algorithms on the data
###Random forest
```{r}


#I will use 5-fold cross-validation as a method to optimise the mtry parameter of the random forest

control <- trainControl(method = "cv", number = 5, verboseIter = T, savePredictions = T)

rf_grid <- expand.grid(mtry = c(4:25))

set.seed(100)
rf_optimized <- train(group~., data = training_set %>% select(group, numeric_features) , 
                      trControl = control, method = "rf", 
                              tuneGrid = rf_grid, verbose = T)

#The accuracy for mtry = 13 is 85.6 %

#I will try adding more trees to the forest.

set.seed(100)
rf_optimized <- train(group~., data = training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                              tuneGrid = rf_grid, verbose = T, ntree = 1000)

#There is no difference in the model accuracy, the highest is 85.6 %. 

set.seed(100)
rf_optimized <- train(group~., data = training_set %>% select(group, numeric_features) , trControl = control, method = "rf", 
                              tuneGrid = expand.grid(mtry = 13), verbose = T)

```
###Svm
SVM are also suitable for multivariate classification. I will test if the performance is better than the random forest. 
```{r}

#SVM are very sensitive to redundant features, therefore I will train the algorithm only on the most important features for the classifier. 

set.seed(100)
svm_radial <- train(group~., data = training_set %>% select(group, imp_feat_initial_rf), trControl = control, method = "svmRadial", 
                         tuneGrid  = expand.grid(C =c(0.25, 0.5, 0.75,1,1.5,2,5,10,15,20), sigma = seq(0.01,0.2, by = 0.025)))

#The highest accuracy is 79.1 %

```
##Confusion matrix
A random forest with mtry = 13 was the most accurate algorithm. I will make a confusion matrix to investigate the classification error of the different groups. 
```{r}

saveRDS(rf_optimized, file = "objects_classification/crowded_state_classifier.rds")

conf_optimal_rf <- table(rf_optimized$pred$obs, rf_optimized$pred$pred) %>% as.data.frame() %>% arrange(Var1, Var2) %>% group_by(Var1) %>% 
                      mutate(proportion = Freq/sum(Freq), Var2 = factor(Var2, levels = c("Normal", "Irregular_nucleus", "Condensed", "Big")))
  

load("~/Desktop/Sergi_master_thesis/themes/confusion_matrix_theme.RData")

confusion_matrix <- ggplot(conf_optimal_rf, aes(x = Var1, y = Var2, fill = proportion))+
                                 geom_tile()+
                                 scale_x_discrete(expand = c(0,0), labels = c("Big", "Condensed","Irregular\nnucleus", "Normal"))+
                                 scale_y_discrete(expand = c(0,0), labels = c("Normal", "Irregular\nnucleus", "Condensed", "Big"))+
                                 scale_fill_gradient(low = "white", high = "#4885ed")+
                                 xlab("user-defined group")+
                                 ylab("predicted group")+
                                 ggtitle("Confusion matrix (proportion of cells)")+
                                 confusion_matrix_theme()+
                                 #theme(axis.text.x = element_text(angle = 45, hjust = 1))+
                                 geom_label(data = conf_optimal_rf %>% filter(proportion>0.1), 
                                            aes(x = Var1, y = Var2, label = round(proportion, 2)), size = 6, label.size = 0.5)

ggsave(confusion_matrix, filename = "objects_classification/plots/confusion_matrix.pdf", height = 7, width = 9)
       
#All classes have an accuracy above 80 %. The most frequent mistake is to assign a big cells to the normal group. 
```
#Supp Figures thesis

In the supplementaries of the thesis I will add a plot for every classification algorithm, which will include the confusion matrix and a table of the training set composition. In case I did relabelling it could be interesting to add the initial and final confusion matrices with the corresponding training set tables. 


I will make two tables one which will show the initial group composition of the training set and the final. The other showing the percentage of relabeled and discarded cells after the reinspection step. 

For the Crowded and Isolated Classifiers it might be interesting to also include a UMAP with the group distribution of the cells. 

```{r}


umap_theme_thesis <- function(){
  
  theme_classic()+
    theme(legend.text = element_text(size = 19),
          axis.title.x = element_text(size = 16),
          axis.title.y = element_text(size = 16),
          legend.position = "bottom",
          legend.spacing.x = unit(1, "cm"),
          plot.title = element_blank(),
          axis.title = element_text(size = 13),
          legend.title = element_blank())

}


train_table <- numbers_table %>% select(group, number_cells) %>% 
                  mutate(group = if_else(group == "Irregular_nucleus", "Irregular nucleus", group))

colnames(train_table) <- c("Group", "Number cells")


supp_figure_arrnged <- grid.arrange(arrangeGrob(confusion_matrix + 
                                                  theme(legend.position = "bottom") +
                                                guides(fill = guide_colourbar(
                                                                       title.hjust = 0.5,
                                                                       title.vjust = 0.9,
                                                                      barwidth = unit(5, "cm"),
                                                                      ))+
                                                labs(fill = "Proportion of cells")),
                                    arrangeGrob(tableGrob(train_table, rows = NULL, 
                                                   theme = ttheme_default(base_size = 15))), 
                                    ncol = 2, widths = c(1.25, 1))
                                    
supp_figure_arrnged <-grid.arrange(arrangeGrob(supp_figure_arrnged),
                                   arrangeGrob(umap_imp_plot+
                                                 theme(legend.title = element_blank())+
                                     guides(colour = guide_legend(nrow = 1, byrow = T,
                                                                override.aes = list(size = 5)))+
                                     umap_theme_thesis()), nrow = 2, heights = c(1,0.9)) 

ggsave(supp_figure_arrnged, filename = "plots_thesis/supp_figure10_conf.pdf", height = 13, 
       width = 13)

#The UMAP looks awful cause it spans the figure width. That is why I'll make an arranged figure in
#which the UMAP looks better but the conf. matrix and the tables do not. Then I will arrange them 
#inkscape.

supp_figure_arrnged_umap <-grid.arrange(arrangeGrob(supp_figure_arrnged),
                                   arrangeGrob(umap_imp_plot+
                                                 theme(legend.title = element_blank())+
                                     guides(colour = guide_legend(nrow = 1, byrow = T,
                                                                override.aes = list(size = 5)))+
                                     umap_theme_thesis()), nrow = 2, heights = c(1,0.9),
                                   respect = T)  


ggsave(supp_figure_arrnged_umap, filename = "plots_thesis/supp_figure10_umap.pdf", height = 13, 
       width = 13)

```